{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis Using linear regression\n",
    "1. Download and import dataset\n",
    "2. Clean, rename columns, check data\n",
    "3. Tokenize words\n",
    "4. Train model\n",
    "5. Check accuracy\n",
    "6. Run AUC, F1\n",
    " \n",
    " \n",
    "**Extras**\n",
    "- Build Binary classifier (positive/negative)\n",
    "- Interpet results\n",
    "- Print words with the most positive and negative weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -nc https://lazyprogrammer.me/course_files/AirlineTweets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import f1_score, roc_auc_score, roc_curve, auc, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5 # confidence in sentiment threshold \n",
    "\n",
    "def process_string(s):\n",
    "    # Split the string on spaces\n",
    "    words = s.split()\n",
    "    \n",
    "    # Remove first element\n",
    "    words.pop(0)\n",
    "    \n",
    "    # Rejoin the list into a string\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "df = pd.read_csv('AirlineTweets.csv',delimiter=\",\")\n",
    "df = df[df['airline_sentiment_confidence']>threshold]\n",
    "df = df[['airline_sentiment','text']]\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "df['text'] = df['text'].apply(process_string)\n",
    "\n",
    "sentiment = df[\"airline_sentiment\"]\n",
    "text = df[\"text\"]\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "\n",
    "## Convert classes to numbers\n",
    "#target_map = {\"pos\":1,\"neg\":0,\"neut\":2}\n",
    "#df['airline_sentiment'] = df['airline_sentiment'].map(target_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: create a function that splits the data but keeps the same percentage of each class in both train and test\n",
    "# Check for class imbalance\n",
    "\n",
    "test_plots = df['airline_sentiment'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text, sentiment, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True)\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# function .fit_transform() trains on a new vocabulary, \n",
    "# while .transform() trains on existing vocabulary\n",
    "X_train_vect = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_vect = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000,class_weight='balanced')\n",
    "model.fit(X_train_vect, y_train)\n",
    "print(\"Model train acc:\",model.score(X_train_vect,y_train))\n",
    "print(\"Model test acc:\",model.score(X_test_vect,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict_proba(X_test_vect)\n",
    "probs = [\"negative\",\"neuteral\",\"positive\"]\n",
    "\n",
    "negative_sentiment = {}\n",
    "neuteral_sentiment = {}\n",
    "positive_sentiment = {}\n",
    "\n",
    "\n",
    "for i,pred in enumerate(preds):\n",
    "  \n",
    "    index = np.argmax(pred)\n",
    "    \n",
    "    if index == 0:\n",
    "        negative_sentiment[X_test.iloc[i]] = pred[index]\n",
    "    \n",
    "    if index == 1:\n",
    "        neuteral_sentiment[X_test.iloc[i]] = pred[index]\n",
    "    \n",
    "    if index == 2:\n",
    "        positive_sentiment[X_test.iloc[i]] = pred[index]\n",
    "    \n",
    "        \n",
    "# get most positive review\n",
    "max_key = max(positive_sentiment, key=positive_sentiment.get)\n",
    "print(max_key)\n",
    "print(positive_sentiment[max_key])\n",
    "\n",
    "\n",
    "# get most negative review\n",
    "max_key = max(negative_sentiment, key=negative_sentiment.get)\n",
    "print(max_key)\n",
    "print(negative_sentiment[max_key])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample custom text for prediction\n",
    "custom_text = [\"it was mid\", \"it was awesome\"]\n",
    "\n",
    "# Transform the custom text using the same vectorizer\n",
    "custom_text_vectorized = tfidf_vectorizer.transform(custom_text)\n",
    "\n",
    "# Make the prediction\n",
    "predictions = model.predict(custom_text_vectorized)\n",
    "\n",
    "# Display the predictions\n",
    "for text, prediction in zip(custom_text, predictions):\n",
    "    print(f\"Text: {text}\\nPrediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate AUC Score\n",
    "P_train = model.predict_proba(X_train_vect)\n",
    "P_test = model.predict_proba(X_test_vect)\n",
    "print(\"Train AUC:\",roc_auc_score(y_train,P_train,multi_class='ovo'))\n",
    "print(\"Test AUC:\",roc_auc_score(y_test,P_test,multi_class='ovo'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "P_train = model.predict(X_train_vect)\n",
    "P_test = model.predict(X_test_vect)\n",
    "cm = confusion_matrix(y_train,P_train,normalize='true')\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=probs, yticklabels=probs)\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test,P_test,normalize='true')\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=probs, yticklabels=probs)\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Binary classifier\n",
    "Keep only negative and positive classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_list =  df[df['airline_sentiment'] != \"neutral\"]\n",
    "\n",
    "text = binary_list['text']\n",
    "sentiment = binary_list['airline_sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, sentiment, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(lowercase=True,max_features=2000)\n",
    "\n",
    "# function .fit_transform() trains on a new vocabulary, \n",
    "# while .transform() trains on existing vocabulary\n",
    "X_train_vect = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_vect = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vect, y_train)\n",
    "print(\"Model train acc:\",model.score(X_train_vect,y_train))\n",
    "print(\"Model test acc:\",model.score(X_test_vect,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate AUC Score\n",
    "P_train = model.predict_proba(X_train_vect)[:,1]\n",
    "P_test = model.predict_proba(X_test_vect)[:,1]\n",
    "print(\"Train AUC:\",roc_auc_score(y_train,P_train))\n",
    "print(\"Test AUC:\",roc_auc_score(y_test,P_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "P_train = model.predict(X_train_vect)\n",
    "P_test = model.predict(X_test_vect)\n",
    "cm = confusion_matrix(y_train,P_train,normalize='true')\n",
    "\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=probs, yticklabels=probs)\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.coef to obtain the wÎµights\n",
    "plt.hist(model.coef_[0],bins=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word to index dictionary\n",
    "word_index_map = tfidf_vectorizer.vocabulary_\n",
    "word_index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the weghts of each words\n",
    "threshold = 2\n",
    "print(\"most positive words\")\n",
    "\n",
    "all_weights = []\n",
    "all_wwords = []\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    all_weights.append(weight)\n",
    "    all_wwords.append(word)\n",
    "    if weight > threshold:\n",
    "        print(word,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.DataFrame(all_wwords,columns=['words'])\n",
    "all_df['weights'] = all_weights\n",
    "df_sorted = all_df.sort_values(by='weights',ascending=False)\n",
    "df_sorted.head(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the weghts of each words\n",
    "threshold = 2\n",
    "print(\"most positive words\")\n",
    "\n",
    "for word, index in word_index_map.items():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight < -threshold:\n",
    "        print(word,weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_string = 'del'\n",
    "filtered_df = all_df[all_df['words'].str.contains(search_string, case=False, na=False)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the most wrong tweets for both classes\n",
    "# find the most false positive with most confidence\n",
    "\n",
    "Preds = model.predict_proba(X_test_vect)\n",
    "pred_conf = np.amax(Preds,axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "get_wrongest_preds = pd.DataFrame(pred_conf,columns = ['preds'] )\n",
    "get_wrongest_preds['Y'] = y_test\n",
    "get_wrongest_preds['X_test'] = X_test\n",
    "get_wrongest_preds\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
