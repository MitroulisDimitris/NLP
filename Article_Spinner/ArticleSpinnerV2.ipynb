{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File 'bbc_text_cls.csv' already there; not retrieving.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dimitris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import textwrap\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "# download punctuation\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import dataset\n",
    "df = pd.read_csv('bbc_text_cls.csv')#, on_bad_lines='skip')\n",
    "df.head()\n",
    "\n",
    "labels = set(df['labels'])\n",
    "\n",
    "### CONSTANTS ####\n",
    "\n",
    "# {'politics', 'sport', 'tech', 'business', 'entertainment'}\n",
    "label = 'politics' # train only from chosen labels\n",
    "context_window = 1 # How many words before and after to take as a context\n",
    "change_prob = 0.3 # Propability to change a word  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_counts(texts):\n",
    "    # collect counts\n",
    "\n",
    "    probs = {} # key: [(wt-1), (wt+1)] value: [ wt / count(wt)]\n",
    "\n",
    "    for doc in texts:\n",
    "        lines = doc.split(\"\\n\")\n",
    "        for line in lines:\n",
    "            tokens = word_tokenize(line)\n",
    "            # for lenght of tokens - context window \n",
    "            for i in range(len(tokens)-(context_window*2+1)):\n",
    "                # for context window\n",
    "                t = []\n",
    "                for j in range(context_window*2 + 1):\n",
    "                    t.append(tokens[i+j])\n",
    "                    \n",
    "                wt = t[len(t)//2]\n",
    "                del[t[len(t)//2]]\n",
    "                key = tuple(t)\n",
    "                \n",
    "                if key not in probs:\n",
    "                    probs[key] = {}\n",
    "                    \n",
    "                # add count for middle token\n",
    "                if wt not in probs[key]:\n",
    "                    probs[key][wt] = 1\n",
    "                else: \n",
    "                    probs[key][wt] += 1\n",
    "    return probs\n",
    "  \n",
    "  \n",
    "def convert_to_probs(probs):\n",
    "    for key, d in probs.items():\n",
    "        # d should represent a distibution\n",
    "        total = sum(d.values())\n",
    "        # access the dictionary of dictionaries, in corresponding key\n",
    "        # devide curernt count with total\n",
    "        for k, v in d.items():\n",
    "            d[k] = v/total  \n",
    "            \n",
    "    return probs\n",
    "        \n",
    "        \n",
    "def sample_word(d):\n",
    "    p0 = np.random.random()\n",
    "    cumulative = 0\n",
    "    for t,p in d.items():\n",
    "        cumulative += p\n",
    "        if p0 < cumulative:\n",
    "            return t\n",
    "    assert(False)        \n",
    "    \n",
    "def spin_line(line,probs):\n",
    "    tokens = word_tokenize(line)\n",
    "    i = 0\n",
    "    output = [tokens[0]]\n",
    "    \n",
    "\n",
    "    while i <(len(tokens)-(context_window*2+1)):\n",
    "        t = []\n",
    "        for j in range(context_window*2 + 1):\n",
    "            t.append(tokens[i+j])\n",
    "      \n",
    "        wt = t[len(t)//2]\n",
    "        del[t[len(t)//2]]\n",
    "        key = tuple(t)\n",
    "        \n",
    "        \n",
    "        p_dist = probs[key]\n",
    "\n",
    "        if len(p_dist) > 1 and np.random.random() < change_prob:\n",
    "            # replace middle word\n",
    "            middle = sample_word(p_dist)\n",
    "            output.append(wt)\n",
    "            output.append(\"<\"+middle+\">\")\n",
    "            output.append(t[len(t)//2])\n",
    "            \n",
    "            # skip 2 steps \n",
    "            i +=2\n",
    "        \n",
    "        else:\n",
    "            # dont replace middle word\n",
    "            output.append(wt)\n",
    "            i+=1\n",
    "        \n",
    "        \n",
    "    # append the final token\n",
    "    if i == (len(tokens)-2):\n",
    "        output.append(tokens[-1])\n",
    "    return detokenizer.detokenize(output)    \n",
    "\n",
    "\n",
    "def spin_document(doc,probs):\n",
    "    # split document into lines\n",
    "    lines = doc.split(\"\\n\")\n",
    "    output = []\n",
    "    for line in lines:\n",
    "        if line:\n",
    "            new_line = spin_line(line,probs)\n",
    "        else:\n",
    "            new_line = line\n",
    "        output.append(new_line)\n",
    "        \n",
    "    return \"\\n\".join(output)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maternity pay for new mothers is to rise by £1,400 as part of new proposals announced by the Trade and Industry Secretary Patricia Hewitt.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = df[df['labels'] == label]['text']\n",
    "probs = collect_counts(texts)\n",
    "norm_probs = convert_to_probs(probs)\n",
    "\n",
    "# text is split on paragraphs\n",
    "texts.iloc[0].split(\"\\n\")\n",
    "\n",
    "\n",
    "detokenizer = TreebankWordDetokenizer()\n",
    "texts.iloc[0].split(\"\\n\")[2]\n",
    "detokenizer.detokenize(word_tokenize(texts.iloc[0].split(\"\\n\")[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UKIP outspent Labour on\n",
      "\n",
      "The UK Independence Party outspent both\n",
      "Labour and the Liberal Democrats in the European elections <history>,\n",
      "new figures\n",
      "\n",
      "UKIP, which campaigned <starts> on a slogan <way> of \"Say\n",
      "no to Europe\" <has>, spent £2.36m on the campaign <party> - second\n",
      "<that> only to the Conservatives' £3.13m <manifesto>. The campaign\n",
      "took UKIP into third place with an extra 10 MEPs . Labour's campaign\n",
      "cost £1.7m, the <the> Lib Dems' £1.19m and the Greens' £404,000,\n",
      "according to figures revealed by the Electoral <European> Commission\n",
      "on Wednesday . Much of the UKIP funding came from Yorkshire\n",
      "millionaire Sir Paul Sykes, who helped bankroll <make> the party's\n",
      "billboard campaign . Critics <We> have accused <seen> the party of\n",
      "effectively buying votes . But <In> a UKIP <Labour> spokesman said\n",
      "Labour and the Conservatives <Treasury> had spent £10m between them on\n",
      "the last <next> general election . \"With the advantages of public\n",
      "<wasting> money the others <women> have, the only way the smaller\n",
      "parties can get their message across is by buying <putting> the\n",
      "advertising space,\" he\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(1234)\n",
    "i = np.random.choice(texts.shape[0])\n",
    "doc = texts.iloc[i]\n",
    "new_doc = spin_document(doc,norm_probs)\n",
    "print(textwrap.fill(new_doc, replace_whitespace=False,fix_sentence_endings=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
